{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = ''\n",
    "out_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import time\n",
    "# import cv2.bgsegm\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Get process ID\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "\n",
    "cooldown_period = 300\n",
    "last_trespass_alert_time = 0\n",
    "\n",
    "\n",
    "# Initialize background subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=False)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)) \n",
    "def send_email_alert(\n",
    "    sender = \"your_email@example.com\",\n",
    "    receiver = \"authority@example.com\",\n",
    "    subject = \"Trespassing Alert\",\n",
    "    body = \"A human trespassing event was detected at [Camera_ID] on [Timestamp].\"\n",
    "):\n",
    "    global last_trespass_alert_time\n",
    "    current_time = time.time()\n",
    "    \n",
    "    if current_time - last_trespass_alert_time >= cooldown_period:\n",
    "        last_trespass_alert_time = current_time\n",
    "        msg = MIMEText(body)\n",
    "        msg['Subject'] = subject\n",
    "        msg['From'] = sender\n",
    "        msg['To'] = receiver\n",
    "        try:\n",
    "            server = smtplib.SMTP(\"smtp.example.com\", 587)  # Change host and port as needed\n",
    "            server.starttls()\n",
    "            server.login(sender, \"your_email_password\")\n",
    "            server.sendmail(sender, [receiver], msg.as_string())\n",
    "            server.quit()\n",
    "            print(\"Alert email sent.\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to send email:\", e)\n",
    "\n",
    "# Sample loop for processing video frames\n",
    "cap = cv2.VideoCapture(video_path)  # Replace with your video source\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgMask = backSub.apply(frame)\n",
    "    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel)\n",
    "    # Additional processing: noise reduction and thresholding (details omitted for brevity)\n",
    "    # Identify contours in the foreground mask\n",
    "    contours, _ = cv2.findContours(fgMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        if 1000 > cv2.contourArea(contour) > 800:  # Set area threshold to ignore small movements\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            aspect_ratio = h / float(w)\n",
    "            # A simple check: humans tend to have an aspect ratio greater than 1.2\n",
    "            if aspect_ratio > 1.2:\n",
    "                # Optionally run a Haar-cascade human detector on this region if needed\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                # print(\"Human-like motion detected.\")\n",
    "                # Trigger the email alert (ensure you throttle alerts to prevent spamming)\n",
    "                # send_email_alert()\n",
    "                # print(\"\\n\\nDETECTED\\n\\n\")\n",
    "                break  # Trigger one alert and continue processing\n",
    "\n",
    "    # Display both fgMask and frame side by side\n",
    "    cv2.imshow(\"FG Mask\", fgMask)\n",
    "    cv2.moveWindow(\"FG Mask\", 0, 0)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.moveWindow(\"Frame\", 640, 0)\n",
    "    # cv2.imshow(\"Frame\", frame)\n",
    "    # time.sleep(1)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Get memory usage in MB\n",
    "memory_used = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Peak Memory Usage: {memory_used:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Recognition\n",
    "\n",
    "# Importing the libraries\n",
    "import cv2\n",
    "\n",
    "# Loading the cascades\n",
    "face_cascade = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('Haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "# Defining a function that will do the detections\n",
    "def detect(gray, frame):\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.3, 6)\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "# Doing some Face Recognition with the webcam\n",
    "video_capture = cv2.VideoCapture(video_path)  # Replace with your video source\n",
    "# video_capture = cv2.VideoCapture()\n",
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    if not _: break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    canvas = detect(gray, frame)\n",
    "    cv2.imshow('Video', canvas)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Get process ID\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "cooldown_period = 300\n",
    "last_trespass_alert_time = 0\n",
    "\n",
    "# Initialize background subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=False)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)) \n",
    "\n",
    "# Video source\n",
    "input_video_path = video_path  # Replace with your video path\n",
    "output_video_path = out_path  # Path to save processed video\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties (width, height, fps)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'XVID' for AVI\n",
    "# out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "output_width = frame_width * 2  # Double the width for side-by-side display\n",
    "output_height = frame_height    # Height remains the same\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (output_width, output_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgMask = backSub.apply(frame)\n",
    "    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel)\n",
    "    fgMask[fgMask == 127] = 0  # Remove shadows\n",
    "    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_CLOSE, kernel)  # Remove noise\n",
    "\n",
    "    # Identify contours\n",
    "    contours, _ = cv2.findContours(fgMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        if 1000 > cv2.contourArea(contour) > 800:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            aspect_ratio = h / float(w)\n",
    "            if aspect_ratio > 1.2:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                # print(\"Human-like motion detected.\")\n",
    "                break  \n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    fgMask_colored = cv2.cvtColor(fgMask, cv2.COLOR_GRAY2BGR)  # Convert grayscale fgMask to 3-channel\n",
    "    combined_frame = cv2.hconcat([fgMask_colored, frame])  # Stack horizontally\n",
    "\n",
    "    out.write(combined_frame)\n",
    "\n",
    "    # Display frames\n",
    "    cv2.imshow(\"FG Mask\", fgMask)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()  # Save the video\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Get memory usage in MB\n",
    "memory_used = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Peak Memory Usage: {memory_used:.2f} MB\")\n",
    "print(f\"Processed video saved at: {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(video_path)  # Replace with your video\n",
    "\n",
    "# Initialize Background Subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)\n",
    "\n",
    "# Initialize HOG descriptor + SVM for human detection\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Morphological kernel (to reduce noise)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgMask = backSub.apply(gray)\n",
    "    fgMask[fgMask == 127] = 0  # Remove shadows\n",
    "    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_CLOSE, kernel)  # Reduce noise\n",
    "\n",
    "    # Find contours of moving objects\n",
    "    contours, _ = cv2.findContours(fgMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    human_detected = False\n",
    "\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 800:  # Ignore small movements\n",
    "            continue\n",
    "\n",
    "        # Get bounding box of moving object\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        # Ensure the extracted moving region is valid\n",
    "        if w > 64 and h > 128:  # HOG requires at least 64x128 pixels to detect humans\n",
    "            moving_region = frame[y:y+h, x:x+w]  # Extract moving object\n",
    "            humans, _ = hog.detectMultiScale(moving_region, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "            for (hx, hy, hw, hh) in humans:\n",
    "                human_detected = True\n",
    "                cv2.rectangle(frame, (x+hx, y+hy), (x+hx+hw, y+hy+hh), (0, 255, 0), 2)  # Draw human box\n",
    "    # Print message if human motion is detected\n",
    "    if human_detected:\n",
    "        print(\"🚨 Human motion detected!\")\n",
    "\n",
    "    # Display results\n",
    "    cv2.imshow(\"Foreground Mask\", fgMask)\n",
    "    cv2.imshow(\"Motion Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load YOLOv8 model (smallest version)\n",
    "model = YOLO(\"yolov8n.pt\")  # \"n\" means Nano, fastest model\n",
    "\n",
    "# Open video stream\n",
    "cap = cv2.VideoCapture(video_path)  # Use 0 for webcam or video file path\n",
    "\n",
    "i = 1\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if i%5 != 0:\n",
    "        i+=1\n",
    "        continue\n",
    "    i=1\n",
    "    # Run YOLOv8 detection\n",
    "    results = model(frame)\n",
    "\n",
    "    human_detected = False\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            if box.cls == 0:  # Class 0 in YOLO is \"person\"\n",
    "                human_detected = True\n",
    "                # print(box)\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Print result\n",
    "    # print(\"🚨 Human detected!\" if human_detected else \"❌ No human detected.\")\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLOv8 Human Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
